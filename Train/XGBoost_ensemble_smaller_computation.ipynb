{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["\u003ca href=\"https://colab.research.google.com/github/Engineer-Ayesha-Shafique/Brain-Tumor-Segmentation-and-Detection-using-UNET-and-Watershed-in-Python/blob/main/Brain_Tumor_Segmentation_and_Detection_using_UNET_and_Watershed_in_Python.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16145,"status":"ok","timestamp":1745672915034,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"v0LHr_cN94aF","outputId":"a6a844ca-5b30-4272-f050-9ede5d756536"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive' ,force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"Yacw50PTTLCQ"},"source":["#MOE\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1745672915037,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"8aJ2oKaYnd1M"},"outputs":[],"source":["import os\n","BASE_MODEL_PATH = \"/content/drive/MyDrive/Project_41725\"\n","MODEL_1 = os.path.join(BASE_MODEL_PATH, \"files\" , \"UNetModified\" , \"model.h5\")\n","MODEL_2 = os.path.join(BASE_MODEL_PATH, \"files\" , \"LinkNet\" , \"model.h5\")\n","MODEL_3 = os.path.join(BASE_MODEL_PATH, \"files\", \"aug_pass\" , \"model_512.h5\")\n","MODEL_4 = os.path.join(BASE_MODEL_PATH, \"files\", \"DG\" , \"model.h5\")\n","MODEL_5 = os.path.join(BASE_MODEL_PATH, \"files\", \"DG\" , \"model_256.h5\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4270,"status":"ok","timestamp":1745672919309,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"wNQiO38M8v-4","outputId":"56bc14ff-2a18-47c3-e37d-6cf373937840"},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ TensorFlow will use 1 GPU(s)\n"]}],"source":["import tensorflow as tf\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    for gpu in gpus:\n","        tf.config.experimental.set_memory_growth(gpu, True)\n","    print(f\"✅ TensorFlow will use {len(gpus)} GPU(s)\")\n","else:\n","    print(\"⚠️ No GPU detected for TensorFlow\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":524,"status":"ok","timestamp":1745672919840,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"1ixsseaB8uIZ"},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from xgboost import XGBClassifier\n","from tqdm import tqdm\n","import imageio"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1745672919900,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"jwPlOqg0q_bu"},"outputs":[],"source":["# ── 1) Imports \u0026 constants ─────────────────────────────────────────────────────\n","\n","import os\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import (\n","    Input, Conv2D, Activation, Lambda\n",")\n","\n","INPUT_SHAPE = (224, 224, 3)\n","NUM_CLASSES = 5"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3752,"status":"ok","timestamp":1745672923654,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"VHzsjqsR5HMl","outputId":"469a80ae-1f21-4880-d26f-5cd732f7facb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 7.5.0 requires typeguard\u003e=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install --quiet segmentation-models tensorflow-addons\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1745672923705,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"rQpQpgEf1BWw","outputId":"8d278e27-f1cc-4f03-a8e8-3e144a85dc41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Segmentation Models: using `tf.keras` framework.\n"]}],"source":["import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import segmentation_models as sm\n","from tensorflow.keras import Model, Input\n","from tensorflow.keras.layers import (\n","    Conv2D, BatchNormalization, Activation,\n","    MaxPool2D\n",")\n","from tensorflow.keras.applications import EfficientNetB4\n","\n","sm.set_framework('tf.keras')\n","\n","BASE_MODEL_PATH = \"/content/drive/MyDrive/Project_41725\"\n","MODEL_1_UNet = os.path.join(BASE_MODEL_PATH, \"files\" , \"UNetModified\" , \"model.h5\")\n","MODEL_2_LINKNET = os.path.join(BASE_MODEL_PATH, \"files\" ,\"LinkNet\", \"modell.h5\")\n","MODEL_3_AttentionUNet = os.path.join(BASE_MODEL_PATH, \"files\", \"aug_pass\", \"model_512.h5\")\n","MODEL_4_DGNet = MODEL_4\n","MODEL_5_Upp =  MODEL_5\n","INPUT_SHAPE = (512, 512, 3)\n","ds_path = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg\"\n","mask_dir = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/masks\"\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11381,"status":"ok","timestamp":1745672935101,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"sTVIxx_sD755","outputId":"308802ca-8a1d-416e-cccd-6e809a5116a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[GPU] Detected 1 GPU(s).\n","Train: 1216, Val: 348, Test: 174\n"]}],"source":["# In a notebook cell, before any cv2 imports:\n","!pip install --quiet opencv-python-headless\n","import os\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","\n","# 1) Import TF first and set memory growth (fallback to experimental)\n","import tensorflow as tf\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    for gpu in gpus:\n","        try:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        except Exception as e:\n","            print(f\"[GPU] Could not set memory growth: {e}\")\n","    print(f\"[GPU] Detected {len(gpus)} GPU(s).\")\n","\n","# 2) Now import the rest\n","import numpy as np\n","import cv2\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import (\n","    ModelCheckpoint, CSVLogger, ReduceLROnPlateau,\n","    EarlyStopping, TensorBoard\n",")\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from xgboost import XGBClassifier\n","from tqdm import tqdm\n","import imageio\n","\"\"\" Global parameters \"\"\"\n","Height = 512\n","Width  = 512\n","\n","def create_folder(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def load_dataset(path_dataset, val_ratio=0.2, test_ratio=0.1, random_state=42):\n","    img_dir  = os.path.join(path_dataset, \"images\")\n","    mask_dir = os.path.join(path_dataset, \"masks\")\n","\n","    image_exts = ('.png', '.jpg', '.jpeg')\n","    images = sorted([\n","        os.path.join(img_dir, f)\n","        for f in os.listdir(img_dir)\n","        if f.lower().endswith(image_exts)\n","    ])\n","\n","\n","    mask_exts = ('.tif', '.tiff')\n","    masks = sorted([\n","        os.path.join(mask_dir, f)\n","        for f in os.listdir(mask_dir)\n","        if f.lower().endswith(mask_exts)\n","    ])\n","\n","    if not images:\n","        raise RuntimeError(f\"No images found in {img_dir}\")\n","    if not masks:\n","        raise RuntimeError(f\"No masks found in {mask_dir}\")\n","\n","    images_lookup = {\n","        os.path.splitext(os.path.basename(p))[0]: p\n","        for p in images\n","    }\n","#MildDemented_0a7b1321-eba0-40dc-85b8-de34241554a2_jpg.rf.70370fcd9ac0fb82f91d27f558053d84.tif\n","    pairs = []\n","    for m in masks:\n","        base = os.path.splitext(os.path.basename(m))[0]\n","        if base.endswith('_mask'):\n","            root = base[:-5]\n","            img_path = images_lookup.get(root)\n","            if img_path:\n","                pairs.append((img_path, m))\n","\n","    if not pairs:\n","        raise RuntimeError(\"No matching image↔mask pairs found!\")\n","\n","    trainval, test = train_test_split(pairs, test_size=test_ratio, random_state=random_state)\n","    val_frac = val_ratio / (1.0 - test_ratio)\n","    train, val = train_test_split(trainval, test_size=val_frac, random_state=random_state)\n","\n","    train_imgs, train_masks = zip(*train)\n","    val_imgs,   val_masks   = zip(*val)\n","    test_imgs,  test_masks  = zip(*test)\n","\n","    return (\n","        list(train_imgs), list(train_masks),\n","        list(val_imgs),   list(val_masks),\n","        list(test_imgs),  list(test_masks)\n","    )\n","\n","def read_image_file(path_img):\n","    path = path_img.decode('utf-8') if isinstance(path_img, bytes) else path_img\n","    img = cv2.imread(path, cv2.IMREAD_COLOR)\n","    img = cv2.resize(img, (Width, Height)) / 255.0\n","    return img.astype(np.float32)\n","\n","def read_mask_file(path_mask):\n","    path = path_mask.decode('utf-8') if isinstance(path_mask, bytes) else path_mask\n","    mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","    if mask.ndim == 3:\n","        mask = mask[..., 0]\n","    mask = cv2.resize(mask, (Width, Height), interpolation=cv2.INTER_NEAREST)\n","\n","    # ✅ Clip to [0, 4], not [0, 3]:\n","    mask = np.clip(mask, 0, NUM_CLASSES-1)\n","\n","    # Convert to integer class IDs\n","    mask = mask.astype(np.int32)\n","\n","    # Add the channel dim back\n","    return mask[..., None]\n","\n","def tf_parse(path_img, path_mask):\n","    # Tell numpy_function: img→float32, mask→int32\n","    img, mask = tf.numpy_function(\n","        lambda pi, pm: (read_image_file(pi), read_mask_file(pm)),\n","        [path_img, path_mask],\n","        [tf.float32, tf.int32]          # \u003c-- second output is now int32\n","    )\n","    img.set_shape([Height, Width, 3])\n","    mask.set_shape([Height, Width, 1])\n","    return img, mask\n","\n","def tf_dataset(inputs, targets, batch_size=2):\n","    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))\n","    ds = ds.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n","    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return ds\n","\n","def edit_mask_images(mask_dir, func, output_dir=None):\n","    out_dir = output_dir or mask_dir\n","    create_folder(out_dir)\n","    exts = ('.tif')\n","    for fname in os.listdir(mask_dir):\n","        if not fname.lower().endswith(exts):\n","            continue\n","        in_path  = os.path.join(mask_dir, fname)\n","        mask     = cv2.imread(in_path, cv2.IMREAD_UNCHANGED)\n","        edited   = func(mask)\n","        cv2.imwrite(os.path.join(out_dir, fname), edited)\n","    print(f\"[edit_mask_images] Applied edits to masks in {out_dir}\")\n","\n","if __name__ == \"__main__\":\n","    np.random.seed(42)\n","    tf.random.set_seed(42)\n","\n","    create_folder(\"/content/drive/MyDrive/Project_41725/files\")\n","\n","    batch_size = 16\n","    lr         = 1e-4\n","    epochs_num     = 100\n","    path_model = os.path.join(\"/content/drive/MyDrive/Project_41725/files\", \"model.h5\")\n","    path_csv   = os.path.join(\"/content/drive/MyDrive/Project_41725/files\", \"log.csv\")\n","\n","    ds_path = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg\"\n","    train_imgs, train_masks, val_imgs, val_masks, test_imgs, test_masks = load_dataset(ds_path)\n","\n","    print(f\"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n","\n","    train_ds = tf_dataset(train_imgs, train_masks, batch_size=batch_size)\n","    val_ds   = tf_dataset(val_imgs,   val_masks,   batch_size=batch_size)\n","    test_ds  = tf_dataset(test_imgs,  test_masks,  batch_size=batch_size)\n","\n","    # Build, compile and fit your model below:\n","    # model = Unet_model((Height, Width, 3))\n","    # model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coeff])\n","    # callbacks = [ModelCheckpoint(...), ...]\n","    # model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1745672935154,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"REl8w4qgr0Ur","outputId":"5bdd2e77-617d-44a1-c98d-1c2cb473acac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Manifest CSVs written.\n"]}],"source":["import pandas as pd\n","\n","# assuming you’ve re-run the load_dataset cell in XGBoost_ensemble.ipynb\n","# so train_imgs, train_masks, test_imgs are available:\n","\n","pd.DataFrame({'image_path': train_imgs}).to_csv('train_images.csv', index=False)\n","pd.DataFrame({'mask_path':  train_masks}).to_csv('train_masks.csv',  index=False)\n","pd.DataFrame({'image_path': test_imgs }).to_csv('test_images.csv',  index=False)\n","print(\"Manifest CSVs written.\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1745672935348,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"NMrqwAWO8VHo"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import (\n","    ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",")\n","import os\n","# Make sure segmentation_models picks up tf.keras, not plain keras\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import get_custom_objects\n","import segmentation_models as sm\n","\n","# Hyper‑params \u0026 paths\n","NUM_CLASSES = 5\n","INPUT_SHAPE = (512, 512, 3)\n","EPOCHS      = 160\n","path_model  = \"/content/drive/MyDrive/Project_41725/XGBoost/files/model.h5\"\n","path_csv    = \"/content/drive/MyDrive/Project_41725/XGBoost/files/log.csv\"\n","\n","# 0) kill any existing graph/state\n","tf.keras.backend.clear_session()\n","\n","# 3) Define losses \u0026 metrics\n","def dice_coef(y_true, y_pred, smooth=1e-6):\n","    y_true = tf.squeeze(y_true, axis=-1)\n","    y_true_o = tf.one_hot(tf.cast(y_true, tf.int32), depth=NUM_CLASSES)\n","    y_true_f = tf.reshape(y_true_o, [-1, NUM_CLASSES])\n","    y_pred_f = tf.reshape(y_pred,   [-1, NUM_CLASSES])\n","    inter = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n","    denom = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n","    dice_per_class = (2. * inter + smooth) / (denom + smooth)\n","    return tf.reduce_mean(dice_per_class)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1.0 - dice_coef(y_true, y_pred)\n","\n","def combined_loss(y_true, y_pred):\n","    ce = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n","    return ce + dice_loss(y_true, y_pred)\n","\n","class MeanIoUArgMax(tf.keras.metrics.MeanIoU):\n","    def __init__(self, num_classes=NUM_CLASSES, name=\"mean_iou\", dtype=None):\n","        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.squeeze(y_true, axis=-1)\n","        y_pred = tf.argmax(y_pred, axis=-1)\n","        return super().update_state(y_true, y_pred, sample_weight)\n","\n","for name, fn in sm.losses.__dict__.items():\n","    get_custom_objects()[name] = fn\n","for name, fn in sm.metrics.__dict__.items():\n","    get_custom_objects()[name] = fn\n","get_custom_objects().update({\n","    'dice_coef': dice_coef,\n","    'combined_loss': combined_loss,\n","    'MeanIoUArgMax': MeanIoUArgMax\n","})\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2516,"status":"ok","timestamp":1745672937868,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"Fq1Wfneir2A9"},"outputs":[],"source":["!pip install --quiet mlxtend xgboost tqdm opencv-python-headless"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":58762,"status":"ok","timestamp":1745672996634,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"VI2JvabUr4DR"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from tqdm import tqdm\n","from tensorflow.keras.models import load_model\n","from sklearn.model_selection import KFold\n","from xgboost import XGBClassifier\n","\n","model1 = load_model(\n","    MODEL_1_UNet,\n","    compile=False\n",")\n","model2 = load_model(\n","    MODEL_2_LINKNET,\n","    compile=False\n",")\n","model3 = load_model(\n","    MODEL_3_AttentionUNet,\n","    compile=False\n",")\n","model4 = load_model(\n","    MODEL_4_DGNet,\n","    compile=False\n",")\n","model5 = load_model(\n","    MODEL_5_Upp,\n","    compile=False\n",")\n","model1 = tf.keras.models.load_model(MODEL_1_UNet, compile=False)\n","model2 = tf.keras.models.load_model(MODEL_2_LINKNET, compile=False)\n","model3 = tf.keras.models.load_model(MODEL_3_AttentionUNet, compile=False)\n","model4 = tf.keras.models.load_model(MODEL_4_DGNet, compile=False)\n","model5 = tf.keras.models.load_model(MODEL_5_Upp, compile=False)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":679},"executionInfo":{"elapsed":219185,"status":"error","timestamp":1745674978791,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"282r_dnHNmj7","outputId":"3199fc69-a51c-4248-a082-19f7cdaca4ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["🔄 Loading base segmentation models…\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea82f162305b4793a7737438ab034dd3","version_major":2,"version_minor":0},"text/plain":["Base models:   0%|          | 0/5 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["🔄 Sampling pixels \u0026 building training set…\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53411b6efebc4352b4472b6c9008d2d7","version_major":2,"version_minor":0},"text/plain":["Images:   0%|          | 0/300 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  → initial X_train: (60000, 25), y_train: (60000,)\n","🔄 Training RandomForest…\n","🔄 Training XGBoost…\n","⏳ Fitting RF…\n","⏳ Fitting XGB…\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:42:54] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["🔄 Building soft-voting ensemble…\n","💾 Saving trained models…\n","✅ Models written to /content/drive/MyDrive/Project_41725/models_new\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:43:02] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n","  warnings.warn(smsg, UserWarning)\n"]},{"ename":"FileNotFoundError","evalue":"segment_image: no such file → /content/drive/MyDrive/Project_41725/newdata/test/image123.png","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-19-ab4542d2287a\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Project_41725/newdata/test/image123.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 176\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ensemble_seg.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-19-ab4542d2287a\u003e\u001b[0m in \u001b[0;36msegment_image\u001b[0;34m(img_path, out_path)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# 0) sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 150\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"segment_image: no such file → {img_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;31m# 1) read \u0026 normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mimg_bgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: segment_image: no such file → /content/drive/MyDrive/Project_41725/newdata/test/image123.png"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from glob import glob\n","from tensorflow.keras.models import load_model\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from xgboost import XGBClassifier\n","from tqdm.auto import tqdm\n","import joblib\n","\n","# ─── CONFIG ────────────────────────────────────────────────────────────────\n","BASE_MODEL_DIR     = \"/content/drive/MyDrive/Project_41725/files\"\n","MODEL_DIR          = \"/content/drive/MyDrive/Project_41725/models_new\"\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","MODEL_PATHS        = [\n","    \"UNetModified/model.h5\",\n","    \"LinkNet/model.h5\",\n","    \"aug_pass/model_512.h5\",\n","    \"DG/model.h5\",\n","    \"DG/model_256.h5\",\n","]\n","IMG_DIR            = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/images\"\n","MSK_DIR            = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/masks\"\n","IMG_EXTS           = (\".png\", \".jpg\", \".jpeg\")\n","MSK_EXTS           = (\".tif\", \".tiff\")\n","SAMPLES_PER_IMAGE  = 200\n","MAX_IMAGES         = 300\n","\n","# ─── LOAD BASE MODELS ─────────────────────────────────────────────────────\n","print(\"🔄 Loading base segmentation models…\")\n","base_models = [\n","    load_model(os.path.join(BASE_MODEL_DIR, p), compile=False)\n","    for p in tqdm(MODEL_PATHS, desc=\"Base models\")\n","]\n","NUM_CLASSES = base_models[0].output_shape[-1]\n","IN_H, IN_W  = base_models[0].input_shape[1:3]\n","\n","# ─── PAIR UP IMAGES ↔ MASKS FOR LATER INJECTION ────────────────────────────\n","# assume mask file is named \"\u003cstem\u003e_mask.tif\" matching \"\u003cstem\u003e.png\"\n","images_lookup = {\n","    os.path.splitext(os.path.basename(p))[0]: p\n","    for ext in IMG_EXTS\n","    for p in glob(os.path.join(IMG_DIR, f\"*{ext}\"))\n","}\n","pairs = []\n","for ext in MSK_EXTS:\n","    for msk_path in glob(os.path.join(MSK_DIR, f\"*{ext}\")):\n","        stem = os.path.splitext(os.path.basename(msk_path))[0]\n","        if stem.endswith(\"_mask\"):\n","            key = stem[:-5]\n","            img_p = images_lookup.get(key)\n","            if img_p:\n","                pairs.append((img_p, msk_path))\n","\n","# ─── FEATURE EXTRACTION ───────────────────────────────────────────────────\n","def get_pixel_probs(img, models):\n","    feats = []\n","    for m in models:\n","        pm = m.predict(img[None], verbose=0)[0]               # (H,W,C)\n","        feats.append(pm.reshape(-1, NUM_CLASSES))            # (H*W,C)\n","    return np.concatenate(feats, axis=1)                     # (H*W, M*C)\n","\n","# ─── SAMPLE TRAINING DATA ─────────────────────────────────────────────────\n","print(\"🔄 Sampling pixels \u0026 building training set…\")\n","train_imgs  = sorted(pairs, key=lambda x: x[0])[:MAX_IMAGES]\n","X_chunks, y_chunks = [], []\n","for img_p, msk_p in tqdm(train_imgs, desc=\"Images\"):\n","    img = cv2.imread(img_p)[..., ::-1] / 255.0\n","    msk = cv2.imread(msk_p, cv2.IMREAD_GRAYSCALE)\n","    img_rs = cv2.resize(img, (IN_W, IN_H), cv2.INTER_AREA)\n","    msk_rs = cv2.resize(msk, (IN_W, IN_H), cv2.INTER_NEAREST).ravel()\n","    P = get_pixel_probs(img_rs, base_models)               # (H*W, M*C)\n","    idxs = np.random.choice(IN_H*IN_W, SAMPLES_PER_IMAGE, replace=False)\n","    X_chunks.append(P[idxs]);  y_chunks.append(msk_rs[idxs])\n","\n","X_train = np.vstack(X_chunks)\n","y_train = np.concatenate(y_chunks)\n","print(f\"  → initial X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","\n","# ─── INJECT MISSING CLASSES ────────────────────────────────────────────────\n","present = set(np.unique(y_train))\n","missing = set(range(NUM_CLASSES)) - present\n","if missing:\n","    print(f\"⚠️ Missing classes: {missing}, injecting one pixel each…\")\n","    for cls in missing:\n","        for img_p, msk_p in pairs:\n","            m_full = cv2.imread(msk_p, cv2.IMREAD_GRAYSCALE)\n","            ys, xs = np.where(m_full == cls)\n","            if len(ys):\n","                # take first occurrence\n","                y0, x0 = ys[0], xs[0]\n","                img_full = cv2.imread(img_p)[..., ::-1] / 255.0\n","                img_rs   = cv2.resize(img_full, (IN_W, IN_H), cv2.INTER_AREA)\n","                P        = get_pixel_probs(img_rs, base_models)\n","                # map full-coord → resized-coord\n","                yr = int(y0 * IN_H / m_full.shape[0])\n","                xr = int(x0 * IN_W / m_full.shape[1])\n","                feat     = P[yr*IN_W + xr]\n","                X_train  = np.vstack([X_train, feat])\n","                y_train  = np.hstack([y_train, cls])\n","                break\n","    print(f\"  → now y_train covers classes: {np.unique(y_train)}\")\n","\n","# ─── TRAIN META-LEARNERS ───────────────────────────────────────────────────\n","print(\"🔄 Training RandomForest…\")\n","rf = RandomForestClassifier(\n","    n_estimators=100,\n","    max_depth=10,\n","    n_jobs=-1,\n","    random_state=42\n",")\n","print(\"🔄 Training XGBoost…\")\n","xgb = XGBClassifier(\n","    objective=\"multi:softprob\",\n","    use_label_encoder=False,\n","    eval_metric=\"mlogloss\",\n","    n_estimators=100,\n","    max_depth=3,\n","    tree_method=\"hist\",\n","    random_state=42\n",")\n","print(\"⏳ Fitting RF…\");  rf.fit(X_train, y_train)\n","print(\"⏳ Fitting XGB…\"); xgb.fit(X_train, y_train)\n","\n","# ─── BUILD SOFT-VOTING ENSEMBLE ───────────────────────────────────────────\n","print(\"🔄 Building soft-voting ensemble…\")\n","ensemble = VotingClassifier(\n","    estimators=[(\"rf\", rf), (\"xgb\", xgb)],\n","    voting=\"soft\",\n","    n_jobs=-1\n",")\n","ensemble.fit(X_train, y_train)\n","\n","# ─── SAVE ALL MODELS ───────────────────────────────────────────────────────\n","print(\"💾 Saving trained models…\")\n","joblib.dump(rf,                     os.path.join(MODEL_DIR, \"rf_meta.pkl\"))\n","xgb.save_model(                     os.path.join(MODEL_DIR, \"xgb_meta.model\"))\n","joblib.dump(ensemble,               os.path.join(MODEL_DIR, \"ensemble_soft_voting.pkl\"))\n","print(\"✅ Models written to\", MODEL_DIR)\n","\n","# ─── UPDATED segment_image WITH PATH CHECK ─────────────────────────────────\n","import os\n","import cv2\n","import numpy as np\n","\n","def segment_image(img_path, out_path=\"seg.png\"):\n","    # 0) sanity check\n","    if not os.path.isfile(img_path):\n","        raise FileNotFoundError(f\"segment_image: no such file → {img_path}\")\n","    # 1) read \u0026 normalize\n","    img_bgr = cv2.imread(img_path)\n","    if img_bgr is None:\n","        raise ValueError(f\"segment_image: failed to decode image → {img_path}\")\n","    img = img_bgr[..., ::-1] / 255.0\n","    H, W = img.shape[:2]\n","\n","    # 2) resize to model input\n","    img_rs = cv2.resize(img, (IN_W, IN_H), interpolation=cv2.INTER_AREA)\n","\n","    # 3) feature extraction \u0026 ensemble predict\n","    P   = get_pixel_probs(img_rs, base_models)    # (IN_H*IN_W, M·C)\n","    lbl = ensemble.predict(P)                      # (IN_H*IN_W,)\n","    mask = lbl.reshape(IN_H, IN_W).astype(np.uint8)\n","\n","    # 4) upsample \u0026 save\n","    mask_full = cv2.resize(mask, (W, H), cv2.INTER_NEAREST)\n","    cv2.imwrite(out_path, mask_full)\n","    print(f\"🔍 Saved segmentation → {out_path}\")\n","    return mask_full\n","\n","\n","# ─── USAGE ─────────────────────────────────────────────────────────────────\n","if __name__==\"__main__\":\n","    test_img = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/\"\n","    _ = segment_image(test_img, out_path=os.path.join(MODEL_DIR,\"ensemble_seg.png\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E809l73-WREc"},"outputs":[],"source":["# ─── 0) INSTALL \u0026 IMPORTS ──────────────────────────────────────────────────\n","!pip install --quiet opencv-python-headless xgboost tqdm\n","\n","from google.colab import files\n","from IPython.display import display\n","import os, cv2, joblib, numpy as np, matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.base import BaseEstimator\n","from tqdm import tqdm\n","\n","# ─── 1) CONFIG ─────────────────────────────────────────────────────────────\n","MODEL_DIR        = \"/content/drive/MyDrive/Project_41725/models_new\"\n","BASE_MODEL_DIR   = \"/content/drive/MyDrive/Project_41725/files\"\n","MODEL_PATHS      = [\n","    \"UNetModified/model.h5\",\n","    \"LinkNet/model.h5\",\n","    \"aug_pass/model_512.h5\",\n","    \"DG/model.h5\",\n","    \"DG/model_256.h5\",\n","]\n","# how many pixels *per class* per image to sample\n","SAMPLES_PER_CLASS_PER_IMAGE = 50\n","\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","# ─── 2) LOAD BASE SEGMENTATION MODELS ───────────────────────────────────────\n","print(\"Loading base segmentation models…\")\n","base_models = []\n","for rel in tqdm(MODEL_PATHS, desc=\"Base models\"):\n","    path = os.path.join(BASE_MODEL_DIR, rel)\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Base model not found: {path}\")\n","    base_models.append(load_model(path, compile=False))\n","\n","# derive some handy constants\n","NUM_CLASSES = base_models[0].output_shape[-1]\n","IN_H, IN_W  = base_models[0].input_shape[1:3]\n","print(f\"→ expecting {NUM_CLASSES} classes, input size {IN_H}×{IN_W}\")\n","\n","# ─── 3) FEATURE EXTRACTOR ───────────────────────────────────────────────────\n","def get_pixel_probs(img_rs):\n","    \"\"\"\n","    img_rs: IN_H×IN_W×3 float32 in [0,1]\n","    returns: (IN_H*IN_W, M * NUM_CLASSES) array of softmax probabilities\n","    \"\"\"\n","    ps = []\n","    for m in base_models:\n","        pm = m.predict(img_rs[None], verbose=0)[0]      # (IN_H,IN_W,NUM_CLASSES)\n","        ps.append(pm.reshape(-1, NUM_CLASSES))         # (IN_H*IN_W,NUM_CLASSES)\n","    return np.concatenate(ps, axis=1)                 # (IN_H*IN_W, M·C)\n","\n","# ─── 4) BALANCED PIXEL SAMPLING \u0026 TRAIN SET BUILDING ───────────────────────\n","from collections import defaultdict\n","\n","IMG_DIR  = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/images\"\n","MSK_DIR  = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/masks\"\n","IMG_EXTS = (\".png\", \".jpg\", \".jpeg\")\n","MSK_EXTS = (\".tif\", \".tiff\")\n","\n","train_imgs  = sorted(sum([list(tqdm.__wrapped__(glob := __import__('glob').glob(\n","    os.path.join(IMG_DIR, f\"*{ext}\")))) for ext in IMG_EXTS], []))\n","train_masks = sorted(sum([list(glob(os.path.join(MSK_DIR, f\"*{ext}\"))) for ext in MSK_EXTS], []))\n","\n","if len(train_imgs) != len(train_masks):\n","    raise RuntimeError(\"Image/mask count mismatch!\")\n","\n","X_per_cls = defaultdict(list)\n","y_per_cls = defaultdict(list)\n","\n","print(\"Sampling pixels per class…\")\n","for img_path, msk_path in tqdm(zip(train_imgs, train_masks),\n","                               total=len(train_imgs),\n","                               desc=\"Images\"):\n","    # load \u0026 prep\n","    img   = cv2.imread(img_path)[..., ::-1].astype(np.float32)/255.0\n","    msk   = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n","    img_rs = cv2.resize(img, (IN_W, IN_H), interpolation=cv2.INTER_AREA)\n","    msk_rs = cv2.resize(msk, (IN_W, IN_H),\n","                        interpolation=cv2.INTER_NEAREST).ravel()\n","    P      = get_pixel_probs(img_rs)  # (IN_H*IN_W, M·C)\n","\n","    for cls in range(NUM_CLASSES):\n","        idxs = np.where(msk_rs == cls)[0]\n","        if len(idxs)==0:\n","            continue\n","        take = np.random.choice(\n","            idxs,\n","            min(SAMPLES_PER_CLASS_PER_IMAGE, len(idxs)),\n","            replace=False\n","        )\n","        X_per_cls[cls].append(P[take])\n","        y_per_cls[cls].append(np.full(len(take), cls, dtype=np.int32))\n","\n","# stack into final X_train, y_train\n","X_list, y_list = [], []\n","for cls in range(NUM_CLASSES):\n","    if X_per_cls[cls]:\n","        X_list.append(np.vstack(X_per_cls[cls]))\n","        y_list.append(np.concatenate(y_per_cls[cls]))\n","\n","X_train = np.vstack(X_list)\n","y_train = np.concatenate(y_list)\n","print(\"  → X_train:\", X_train.shape, \"y_train classes:\", np.unique(y_train))\n","\n","# ─── 5) TRAIN META-LEARNERS \u0026 SAVE ─────────────────────────────────────────\n","print(\"Training RandomForest…\")\n","rf = RandomForestClassifier(n_estimators=100,\n","                            n_jobs=-1,\n","                            random_state=42)\n","rf.fit(X_train, y_train)\n","\n","print(\"Training XGBoost…\")\n","xgb = XGBClassifier(\n","    objective=\"multi:softprob\",\n","    num_class=NUM_CLASSES,\n","    eval_metric=\"mlogloss\",\n","    use_label_encoder=False,\n","    n_jobs=4,\n","    random_state=42\n",")\n","xgb.fit(X_train, y_train)\n","\n","# save them\n","rf_pth  = os.path.join(MODEL_DIR, \"rf_meta.pkl\")\n","xgb_pth = os.path.join(MODEL_DIR, \"xgb_meta.ubj\")\n","joblib.dump(rf, rf_pth)\n","xgb.save_model(xgb_pth)\n","print(f\"✅ Meta-learners saved →\\n  {rf_pth}\\n  {xgb_pth}\")\n","\n","# ─── 6) MANUAL SOFT-VOTING WRAPPER ──────────────────────────────────────────\n","class ManualVotingClassifier(BaseEstimator):\n","    def __init__(self, estimators):\n","        self.models = [m for _,m in estimators]\n","    def predict_proba(self, X):\n","        ps = [m.predict_proba(X) for m in self.models]\n","        return sum(ps)/len(ps)\n","    def predict(self, X):\n","        return np.argmax(self.predict_proba(X), axis=1)\n","\n","vc_meta = ManualVotingClassifier([\n","    (\"rf\", rf),\n","    (\"xgb\", xgb),\n","])\n","\n","# ─── 7) INFERENCE FUNCTION ─────────────────────────────────────────────────\n","def segment_image(img_path, out_path=\"seg.png\"):\n","    if not os.path.isfile(img_path):\n","        raise FileNotFoundError(img_path)\n","    bgr = cv2.imread(img_path)\n","    if bgr is None:\n","        raise ValueError(f\"Bad image: {img_path}\")\n","    rgb = bgr[..., ::-1].astype(np.float32)/255.0\n","    H,W=rgb.shape[:2]\n","    rs=cv2.resize(rgb,(IN_W,IN_H),cv2.INTER_AREA)\n","    P = get_pixel_probs(rs)        # (IN_H*IN_W, M·C)\n","    lbl = vc_meta.predict(P)       # (IN_H*IN_W,)\n","    mask=lbl.reshape(IN_H,IN_W).astype(np.uint8)\n","    full=cv2.resize(mask,(W,H),cv2.INTER_NEAREST)\n","    cv2.imwrite(out_path, full)\n","    return rgb, full\n","\n","# ─── 8) UPLOAD \u0026 RUN GUI ───────────────────────────────────────────────────\n","print(\"▶ Upload one or more MRI slices…\")\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","    print(\"→ Segmenting\", fn)\n","    orig, seg = segment_image(fn, out_path=f\"seg_{fn}\")\n","    fig,axs=plt.subplots(1,2,figsize=(10,5))\n","    axs[0].imshow(orig); axs[0].set_title(\"Input\"); axs[0].axis(\"off\")\n","    axs[1].imshow(seg,cmap=\"gray\"); axs[1].set_title(\"Mask\"); axs[1].axis(\"off\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"elapsed":216955,"status":"ok","timestamp":1745675778725,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"},"user_tz":-180},"id":"1ywt2CwdXx98","outputId":"5b69da65-fe95-4808-e8f4-f21e8cbebbe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://9c79382a62a4719137.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://9c79382a62a4719137.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 \u003c\u003e https://9c79382a62a4719137.gradio.live\n"]},{"data":{"text/plain":[]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# ─── INSTALL ────────────────────────────────────────────────────────────────\n","!pip install --quiet gradio\n","\n","# ─── IMPORTS ────────────────────────────────────────────────────────────────\n","import gradio as gr\n","import cv2, os, numpy as np\n","from sklearn.base import BaseEstimator\n","from xgboost import XGBClassifier\n","import joblib\n","from tensorflow.keras.models import load_model\n","\n","# ─── LOAD META-LEARNERS ─────────────────────────────────────────────────────\n","MODEL_DIR = \"/content/drive/MyDrive/Project_41725/models_new\"\n","xgb_meta  = XGBClassifier();      xgb_meta.load_model(os.path.join(MODEL_DIR, \"xgb_meta.model\"))\n","rf_meta   = joblib.load(os.path.join(MODEL_DIR, \"rf_meta.pkl\"))\n","\n","class ManualVotingClassifier(BaseEstimator):\n","    def __init__(self, estimators):\n","        self.models = [m for _, m in estimators]\n","    def predict_proba(self, X):\n","        probs = [m.predict_proba(X) for m in self.models]\n","        return sum(probs) / len(probs)\n","    def predict(self, X):\n","        return np.argmax(self.predict_proba(X), axis=1)\n","\n","vc_meta = ManualVotingClassifier([(\"xgb\", xgb_meta), (\"rf\", rf_meta)])\n","\n","# ─── LOAD BASE SEGMENTATION MODELS ───────────────────────────────────────────\n","BASE_MODEL_DIR = \"/content/drive/MyDrive/Project_41725/files\"\n","MODEL_PATHS = [\n","    \"UNetModified/model.h5\",\n","    \"LinkNet/model.h5\",\n","    \"aug_pass/model_512.h5\",\n","    \"DG/model.h5\",\n","    \"DG/model_256.h5\",\n","]\n","base_models = [ load_model(os.path.join(BASE_MODEL_DIR,p), compile=False)\n","                for p in MODEL_PATHS ]\n","\n","NUM_CLASSES = base_models[0].output_shape[-1]\n","IN_H, IN_W  = base_models[0].input_shape[1:3]\n","\n","# ─── FEATURE EXTRACTOR ───────────────────────────────────────────────────────\n","def get_pixel_probs(img_rs):\n","    \"\"\"img_rs: IN_H×IN_W×3 float32 → (IN_H*IN_W, M*NUM_CLASSES)\"\"\"\n","    feats = []\n","    for m in base_models:\n","        pm = m.predict(img_rs[None], verbose=0)[0]        # (IN_H,IN_W,NUM_CLASSES)\n","        feats.append(pm.reshape(-1, NUM_CLASSES))\n","    return np.concatenate(feats, axis=1)\n","\n","# ─── CORE INFERENCE ─────────────────────────────────────────────────────────\n","def segment_image_np(img_rgb):\n","    \"\"\"\n","    img_rgb: H×W×3 in RGB uint8\n","    returns: H×W mask uint8\n","    \"\"\"\n","    # normalize \u0026 resize\n","    img_f = img_rgb.astype(np.float32)/255.0\n","    H, W = img_f.shape[:2]\n","    img_rs = cv2.resize(img_f, (IN_W, IN_H), interpolation=cv2.INTER_AREA)\n","    P      = get_pixel_probs(img_rs)                 # (IN_H*IN_W, M·C)\n","    lbl    = vc_meta.predict(P)                      # (IN_H*IN_W,)\n","    mask_rs= lbl.reshape(IN_H, IN_W).astype(np.uint8)\n","    mask   = cv2.resize(mask_rs, (W, H), interpolation=cv2.INTER_NEAREST)\n","    return mask\n","\n","# ─── GRADIO WRAPPER ─────────────────────────────────────────────────────────\n","def gradio_predict(in_img):\n","    # gradio gives H×W×3 in RGB\n","    mask = segment_image_np(in_img)\n","    return in_img, mask\n","\n","iface = gr.Interface(\n","    fn=gradio_predict,\n","    inputs=gr.Image(type=\"numpy\", label=\"Upload MRI (RGB)\"),\n","    outputs=[\n","      gr.Image(type=\"numpy\", label=\"Original\"),\n","      gr.Image(type=\"numpy\", label=\"Segmentation Mask\")\n","    ],\n","    title=\"Brain-Tumor Segmentation Ensemble\",\n","    description=\"Upload an MRI slice → get a 5-model + RF/XGB ensemble mask.\"\n",")\n","\n","iface.launch(debug=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":649},"id":"goDGh5FbYo0-"},"outputs":[{"name":"stdout","output_type":"stream","text":["It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://f846cc9e37eed5af02.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://f846cc9e37eed5af02.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 \u003c\u003e https://f846cc9e37eed5af02.gradio.live\n"]},{"data":{"text/plain":[]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# ─── INSTALL ────────────────────────────────────────────────────────────────\n","!pip install --quiet gradio opencv-python-headless\n","\n","# ─── IMPORTS ────────────────────────────────────────────────────────────────\n","import gradio as gr\n","import cv2, os, numpy as np, joblib\n","from sklearn.base import BaseEstimator\n","from xgboost import XGBClassifier\n","from tensorflow.keras.models import load_model\n","\n","# ─── META-LEARNERS ───────────────────────────────────────────────────────────\n","MODEL_DIR = \"/content/drive/MyDrive/Project_41725/models_new\"\n","xgb_meta  = XGBClassifier(); xgb_meta.load_model(os.path.join(MODEL_DIR, \"xgb_meta.model\"))\n","rf_meta   = joblib.load(os.path.join(MODEL_DIR, \"rf_meta.pkl\"))\n","\n","class ManualVotingClassifier(BaseEstimator):\n","    def __init__(self, estimators):\n","        self.models = [m for _, m in estimators]\n","    def predict_proba(self, X):\n","        probs = [m.predict_proba(X) for m in self.models]\n","        return sum(probs) / len(probs)\n","    def predict(self, X):\n","        return np.argmax(self.predict_proba(X), axis=1)\n","\n","vc_meta = ManualVotingClassifier([(\"xgb\", xgb_meta), (\"rf\", rf_meta)])\n","\n","# ─── BASE SEGMENTATION MODELS ────────────────────────────────────────────────\n","BASE_MODEL_DIR = \"/content/drive/MyDrive/Project_41725/files\"\n","MODEL_PATHS = [\n","    \"UNetModified/model.h5\",\n","    \"LinkNet/model.h5\",\n","    \"aug_pass/model_512.h5\",\n","    \"DG/model.h5\",\n","    \"DG/model_256.h5\",\n","]\n","base_models = [\n","    load_model(os.path.join(BASE_MODEL_DIR, p), compile=False)\n","    for p in MODEL_PATHS\n","]\n","NUM_CLASSES = base_models[0].output_shape[-1]\n","IN_H, IN_W  = base_models[0].input_shape[1:3]\n","\n","# ─── CLASS NAMES \u0026 PALETTE ──────────────────────────────────────────────────\n","class_names = {\n","    0: \"background\",\n","    1: \"glioma\",\n","    2: \"mengoma\",\n","    3: \"pituitary\",\n","    4: \"dementia\"\n","}\n","palette = {\n","    0: (  0,   0,   0),  # black\n","    1: (255,   0,   0),  # red\n","    2: (  0, 255,   0),  # green\n","    3: (  0,   0, 255),  # blue\n","    4: (255, 255,   0)   # yellow\n","}\n","\n","# ─── FEATURE EXTRACTOR ───────────────────────────────────────────────────────\n","def get_pixel_probs(img_rs):\n","    feats = []\n","    for m in base_models:\n","        pm = m.predict(img_rs[None], verbose=0)[0]         # (H,W,C)\n","        feats.append(pm.reshape(-1, NUM_CLASSES))          # (H*W,C)\n","    return np.concatenate(feats, axis=1)                  # (H*W, M·C)\n","\n","# ─── SEGMENT \u0026 STATS ────────────────────────────────────────────────────────\n","def segment_and_stats(img_rgb):\n","    # normalize \u0026 resize\n","    img_f  = img_rgb.astype(np.float32) / 255.0\n","    H, W   = img_f.shape[:2]\n","    img_rs = cv2.resize(img_f, (IN_W, IN_H), interpolation=cv2.INTER_AREA)\n","\n","    # predict per-pixel class\n","    P      = get_pixel_probs(img_rs)\n","    lbl_rs = vc_meta.predict(P).reshape(IN_H, IN_W).astype(np.uint8)\n","\n","    # upsample to original size\n","    mask   = cv2.resize(lbl_rs, (W, H), interpolation=cv2.INTER_NEAREST)\n","\n","    # colorize mask\n","    color_mask = np.zeros((H, W, 3), dtype=np.uint8)\n","    for cls, color in palette.items():\n","        color_mask[mask == cls] = color\n","\n","    # build distribution string\n","    unique, counts = np.unique(mask, return_counts=True)\n","    total = mask.size\n","    lines = []\n","    for u, c in zip(unique, counts):\n","        pct = c/total*100\n","        lines.append(f\"{u} ({class_names[u]}): {c} px — {pct:.2f}%\")\n","    dist_str = \"\\n\".join(lines)\n","\n","    # build HTML legend\n","    legend_items = []\n","    for cls, name in class_names.items():\n","        r,g,b = palette[cls]\n","        hexc = f\"#{r:02x}{g:02x}{b:02x}\"\n","        legend_items.append(\n","            f'\u003cdiv style=\"display:inline-block;margin:4px\"\u003e'+\n","            f'\u003cdiv style=\"width:20px;height:20px;background:{hexc};display:inline-block;\"\u003e\u003c/div\u003e '+\n","            f'\u003cspan\u003eClass {cls}: {name}\u003c/span\u003e\u003c/div\u003e'\n","        )\n","    legend_html = \"\u003cdiv\u003e\" + \"\".join(legend_items) + \"\u003c/div\u003e\"\n","\n","    return color_mask, dist_str, legend_html\n","\n","# ─── GRADIO INTERFACE ───────────────────────────────────────────────────────\n","def gradio_predict(inp_img):\n","    mask_color, stats, legend = segment_and_stats(inp_img)\n","    return inp_img, mask_color, stats, legend\n","\n","iface = gr.Interface(\n","    fn=gradio_predict,\n","    inputs=gr.Image(type=\"numpy\", label=\"Upload MRI (RGB)\"),\n","    outputs=[\n","        gr.Image(type=\"numpy\", label=\"Original\"),\n","        gr.Image(type=\"numpy\", label=\"Segmentation (color)\"),\n","        gr.Textbox(label=\"Per-Class Pixel Distribution\"),\n","        gr.HTML(label=\"Legend\")\n","    ],\n","    title=\"Brain-Tumor Segmentation Ensemble\",\n","    description=\"Drop in an MRI slice to get a colorized 5-class mask, per-class stats, and legend.\"\n",")\n","\n","iface.launch(debug=True)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","name":"","provenance":[{"file_id":"1_k7ogP1xm8XVyrbeskFh20BnYuW2Ycb1","timestamp":1745672745841},{"file_id":"1-VF2VDPbxXDOogmxXajKBr5yhLNQsvtw","timestamp":1745617811043},{"file_id":"1g5ACu-cLTqhufcsgYQON4NX26YE8X6zm","timestamp":1745466123311}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02b49f1853ae4fc5a002efdf857880d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_484314ce9f5b40f7be3553b2cb8e3815","placeholder":"​","style":"IPY_MODEL_eaf0423fbf95484f8fd667d5de6c7074","value":"Base models: 100%"}},"03e095832471456ebab0c5cde3e9dc20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6d7d9bfc5f485893a76e9325eb4d4c","max":300,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d6717f1d4d0458398904d280bc2eb05","value":300}},"0dfcb531c25249538b40825b885fcba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b01de9444583459cae93cf6ba1e599c0","placeholder":"​","style":"IPY_MODEL_48fa5a4d2e3247779371a16e4f65888d","value":" 5/5 [00:12\u0026lt;00:00,  3.07s/it]"}},"1539011ac6d749c8a1a33938afa590a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18421ab1de0949eb94e40874cf8bcb90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26b8a444923841db94dd5ff0ba98af7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a08480b8bd1641aba2db6cc0d7199b9c","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9c83b29ba794b10b287fce2c792a58b","value":5}},"46bb0266594b449ca9dbb64c88ef09cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484314ce9f5b40f7be3553b2cb8e3815":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48fa5a4d2e3247779371a16e4f65888d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49aaebcaacb84e04aef4c74346ef69cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d6717f1d4d0458398904d280bc2eb05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f31a6cee417452683c0d3460fc14c90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53411b6efebc4352b4472b6c9008d2d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eeadaaab17774c18a31ada8bc08e1bcb","IPY_MODEL_03e095832471456ebab0c5cde3e9dc20","IPY_MODEL_6abf5be6d8c84331a0c390cc4c529e50"],"layout":"IPY_MODEL_46bb0266594b449ca9dbb64c88ef09cb"}},"6abf5be6d8c84331a0c390cc4c529e50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49aaebcaacb84e04aef4c74346ef69cb","placeholder":"​","style":"IPY_MODEL_1539011ac6d749c8a1a33938afa590a6","value":" 300/300 [03:14\u0026lt;00:00,  1.99it/s]"}},"a08480b8bd1641aba2db6cc0d7199b9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9c83b29ba794b10b287fce2c792a58b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae6d7d9bfc5f485893a76e9325eb4d4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b01de9444583459cae93cf6ba1e599c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5f25eb2206140178120b4d5ce7ac53c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea82f162305b4793a7737438ab034dd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02b49f1853ae4fc5a002efdf857880d2","IPY_MODEL_26b8a444923841db94dd5ff0ba98af7a","IPY_MODEL_0dfcb531c25249538b40825b885fcba6"],"layout":"IPY_MODEL_4f31a6cee417452683c0d3460fc14c90"}},"eaf0423fbf95484f8fd667d5de6c7074":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeadaaab17774c18a31ada8bc08e1bcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5f25eb2206140178120b4d5ce7ac53c","placeholder":"​","style":"IPY_MODEL_18421ab1de0949eb94e40874cf8bcb90","value":"Images: 100%"}}}}},"nbformat":4,"nbformat_minor":0}