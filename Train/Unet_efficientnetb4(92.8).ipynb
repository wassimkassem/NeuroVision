{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Engineer-Ayesha-Shafique/Brain-Tumor-Segmentation-and-Detection-using-UNET-and-Watershed-in-Python/blob/main/Brain_Tumor_Segmentation_and_Detection_using_UNET_and_Watershed_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0LHr_cN94aF","outputId":"876b91ee-e4bd-404f-9408-8eeaec030a46","executionInfo":{"status":"ok","timestamp":1745609278157,"user_tz":-180,"elapsed":23017,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive' ,force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"Yacw50PTTLCQ"},"source":["#UNET\n","\n","https://www.kaggle.com/datasets/nikhilroxtomar/brain-tumor-segmentation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpVx6L5PU1K3","outputId":"4b66746b-2d9a-4da7-84b4-7e004f4bcf38","executionInfo":{"status":"ok","timestamp":1745414468727,"user_tz":-180,"elapsed":5930,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install tensorflow\n","!pip install --quiet segmentation-models tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["af3817f455a048688649649dc2f4503e","5734b82e3ddf456b97a19bc7b669df2c","59efbe06a0ff4360837cc2ceeb34bb38","b69f5837b97542b287cc40f93176c473","55eda1aacf304ec8a012e53b657c8410","12ba4abe09d942e784b1bdb5c498d8ee","680d104643794616b9d0d7c0cbbb2fdb","27c1467c072c485d8a15d69d88191c8b","73514f6a20ab4a5e89d2fd0b3dc45bf2","657a0fecb2f44634a0182d502bb7673a","803ecbc0461f430aa6625cac3f415407","e70c768cc957404ca12fa5a0865b187e","5da413d85f5c4d9d85877db12d10e0d4","a15e4440ae8f4e58af4e61626de2041d","060172f07e194fe0aaf6ec82aa54e1fb","a7bcda50c3cd4ceea5c85ec42f6b7c53","a484a4780691429fb1a9687a8811c6f2","9e8876065494457985a54ead4fa79013","3130cf13836841be9722452f9bc80f92","478029251a294befa519149018ef4b14","161a9c4fe47246b7ad734b9de1a0c347","eb6c9f4374c646d78fe095e666cd29bc"]},"id":"W5KFDYrrTAye","outputId":"3215ff65-3c6d-4f05-86cd-273dfb3cd28c","executionInfo":{"status":"error","timestamp":1745579285977,"user_tz":-180,"elapsed":22866,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/6.88k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af3817f455a048688649649dc2f4503e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70c768cc957404ca12fa5a0865b187e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFSegformerForSemanticSegmentation: ['decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.batch_norm.num_batches_tracked']\n","- This IS expected if you are initializing TFSegformerForSemanticSegmentation from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFSegformerForSemanticSegmentation from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFSegformerForSemanticSegmentation were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFSegformerForSemanticSegmentation for predictions without further training.\n","Some weights of TFSegformerForSemanticSegmentation were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n","- decode_head.classifier.weight: found shape (150, 256, 1, 1) in the checkpoint and (1, 1, 256, 5) in the model instantiated\n","- decode_head.classifier.bias: found shape (150,) in the checkpoint and (5,) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Exception encountered when calling layer \"tf_segformer_for_semantic_segmentation\" (type TFSegformerForSemanticSegmentation).\n\nin user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 964, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 994, in call  *\n        outputs = self.segformer(\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filembpsv1q3.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filegi5n0z9n.py\", line 15, in tf__call\n        encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileso4txneh.py\", line 103, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n    File \"/tmp/__autograph_generated_fileso4txneh.py\", line 27, in loop_body_1\n        hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n    File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'segformer' (type TFSegformerMainLayer).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 964, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 599, in call  *\n            encoder_outputs = self.encoder(\n        File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileso4txneh.py\", line 103, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n        File \"/tmp/__autograph_generated_fileso4txneh.py\", line 27, in loop_body_1\n            hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n        File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'encoder' (type TFSegformerEncoder).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 516, in call  *\n                hidden_states, height, width = embedding_layer(hidden_states)\n            File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings).\n            \n            in user code:\n            \n                File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 94, in call  *\n                    embeddings = self.proj(self.padding(pixel_values))\n                File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n                    raise ValueError(\n            \n                ValueError: Input 0 of layer \"proj\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 518, 9, 512)\n            \n            \n            Call arguments received by layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings):\n              • pixel_values=tf.Tensor(shape=(None, 512, 3, 512), dtype=float32)\n        \n        \n        Call arguments received by layer 'encoder' (type TFSegformerEncoder):\n          • pixel_values=tf.Tensor(shape=(None, 512, 3, 512), dtype=float32)\n          • output_attentions=False\n          • output_hidden_states=True\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'segformer' (type TFSegformerMainLayer):\n      • pixel_values=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n      • output_attentions=False\n      • output_hidden_states=True\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_segformer_for_semantic_segmentation\" (type TFSegformerForSemanticSegmentation):\n  • pixel_values=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n  • labels=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fcfe878edcb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nvidia/segformer-b0-finetuned-ade-512-512'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     model, prep = build_segformer(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-fcfe878edcb4>\u001b[0m in \u001b[0;36mbuild_segformer\u001b[0;34m(model_name, input_shape, num_labels, activation, trainable_backbone)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# run the HF model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m  \u001b[0;31m# shape: (batch, H, W, num_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;31m# apply activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'segmentation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__run_call_with_unpacked_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mand_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'return_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__run_call_with_unpacked_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'return_dict is not None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mblk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'all_hidden_states'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_self_attentions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_states'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'(idx, x)'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\u001b[0m in \u001b[0;36mloop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitr_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0membedding_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mget_state_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_segformer_for_semantic_segmentation\" (type TFSegformerForSemanticSegmentation).\n\nin user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 964, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 994, in call  *\n        outputs = self.segformer(\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filembpsv1q3.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filegi5n0z9n.py\", line 15, in tf__call\n        encoder_outputs = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(pixel_values),), dict(output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileso4txneh.py\", line 103, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n    File \"/tmp/__autograph_generated_fileso4txneh.py\", line 27, in loop_body_1\n        hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n    File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'segformer' (type TFSegformerMainLayer).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 964, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 599, in call  *\n            encoder_outputs = self.encoder(\n        File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileso4txneh.py\", line 103, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.converted_call(ag__.ld(zip), (ag__.ld(self).embeddings, ag__.ld(self).block, ag__.ld(self).layer_norms), None, fscope),), None, fscope), None, loop_body_1, get_state_4, set_state_4, ('all_hidden_states', 'all_self_attentions', 'hidden_states'), {'iterate_names': '(idx, x)'})\n        File \"/tmp/__autograph_generated_fileso4txneh.py\", line 27, in loop_body_1\n            hidden_states, height, width = ag__.converted_call(ag__.ld(embedding_layer), (ag__.ld(hidden_states),), None, fscope)\n        File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'encoder' (type TFSegformerEncoder).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 516, in call  *\n                hidden_states, height, width = embedding_layer(hidden_states)\n            File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_filehvmbk3rw.py\", line 11, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).proj, (ag__.converted_call(ag__.ld(self).padding, (ag__.ld(pixel_values),), None, fscope),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings).\n            \n            in user code:\n            \n                File \"/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/modeling_tf_segformer.py\", line 94, in call  *\n                    embeddings = self.proj(self.padding(pixel_values))\n                File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n                    raise ValueError(\n            \n                ValueError: Input 0 of layer \"proj\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 518, 9, 512)\n            \n            \n            Call arguments received by layer 'patch_embeddings.0' (type TFSegformerOverlapPatchEmbeddings):\n              • pixel_values=tf.Tensor(shape=(None, 512, 3, 512), dtype=float32)\n        \n        \n        Call arguments received by layer 'encoder' (type TFSegformerEncoder):\n          • pixel_values=tf.Tensor(shape=(None, 512, 3, 512), dtype=float32)\n          • output_attentions=False\n          • output_hidden_states=True\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'segformer' (type TFSegformerMainLayer):\n      • pixel_values=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n      • output_attentions=False\n      • output_hidden_states=True\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_segformer_for_semantic_segmentation\" (type TFSegformerForSemanticSegmentation):\n  • pixel_values=tf.Tensor(shape=(None, 512, 512, 3), dtype=float32)\n  • labels=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","segformer_builder.py\n","\n","Replace UNet builder with a SegFormer-based segmentation model.\n","\n","Dependencies:\n","    pip install tensorflow transformers\n","\n","Usage:\n","    from segformer_builder import build_segformer\n","    model, preprocessor = build_segformer(\n","        model_name='nvidia/segformer-b0-finetuned-ade-512-512',\n","        input_shape=(512,512,3),\n","        num_labels=5\n","    )\n","    model.summary()\n","\n","\"\"\"\n","import tensorflow as tf\n","from transformers import SegformerFeatureExtractor, TFSegformerForSemanticSegmentation\n","\n","\n","def build_segformer(\n","    model_name: str,\n","    input_shape: tuple,\n","    num_labels: int,\n","    activation: str = 'softmax',\n","    trainable_backbone: bool = False\n",") -> tuple:\n","    \"\"\"\n","    Builds a SegFormer segmentation model using Hugging Face Transformers in TensorFlow.\n","\n","    Args:\n","        model_name: name of a pretrained SegFormer checkpoint (e.g. 'nvidia/segformer-b0-finetuned-ade-512-512').\n","        input_shape: (height, width, channels) of input images.\n","        num_labels: number of segmentation classes.\n","        activation: final activation to apply (e.g. 'softmax', 'sigmoid').\n","        trainable_backbone: whether to fine-tune the transformer encoder.\n","\n","    Returns:\n","        model: a tf.keras.Model that accepts BGR uint8 images [0,255] and outputs class logits.\n","        preprocessor: a SegformerFeatureExtractor for input preprocessing.\n","    \"\"\"\n","    # 1) Create the feature extractor for resizing and normalization\n","    preprocessor = SegformerFeatureExtractor(\n","        size={\n","            'height': input_shape[0],\n","            'width': input_shape[1]\n","        },\n","        do_reduce_labels=False,\n","        do_resize=True,\n","        do_normalize=True\n","    )\n","\n","    # 2) Load pretrained SegFormer for semantic segmentation\n","    segformer = TFSegformerForSemanticSegmentation.from_pretrained(\n","        model_name,\n","        num_labels=num_labels,\n","        ignore_mismatched_sizes=True\n","    )\n","\n","    # 3) Build a Keras wrapper\n","    inp = tf.keras.Input(shape=input_shape, dtype=tf.uint8, name='pixel_values')\n","    # preprocess: cast to float and normalize\n","    x = tf.cast(inp, tf.float32)\n","    x = x / 255.0\n","\n","    outputs = segformer(pixel_values=x, training=False).logits  # shape: (batch, H, W, num_labels)\n","\n","    out = tf.keras.layers.Activation(activation, name='segmentation')(outputs)\n","\n","    model = tf.keras.Model(inputs=inp, outputs=out, name='segformer')\n","\n","    if not trainable_backbone:\n","        for layer in model.layers:\n","            if layer.name.startswith('segformer'):  # HF layers prefixed\n","                layer.trainable = False\n","\n","    return model, preprocessor\n","\n","\n","if __name__ == '__main__':\n","    NUM_CLASSES = 5\n","    INPUT_SHAPE = (512, 512, 3)\n","    MODEL_NAME = 'nvidia/segformer-b0-finetuned-ade-512-512'\n","\n","    model, prep = build_segformer(\n","        model_name=MODEL_NAME,\n","        input_shape=INPUT_SHAPE,\n","        num_labels=NUM_CLASSES,\n","        activation='softmax',\n","        trainable_backbone=False\n","    )\n","    model.summary()\n"]},{"cell_type":"code","source":["# install visualkeras for model plotting\n","!pip install --quiet visualkeras\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAXf5xpYyIfM","executionInfo":{"status":"ok","timestamp":1745414482194,"user_tz":-180,"elapsed":1205,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}},"outputId":"89284eae-1a48-4fa1-a71d-10acead5595b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/997.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m993.3/997.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.4/997.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","metadata":{"id":"9ZIpMOX7TbPp"},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTqihBEVCIVY"},"outputs":[],"source":["import tensorflow as tf\n","\n","smooth = 1e-15\n","num_classes = 5  # <-- make this match your final Conv2D(filters=5, ...)\n","\n","def dice_coefficient(y_true, y_pred):\n","    # y_true: (B, H, W, 1) integers 0..4\n","    # y_pred: (B, H, W, 5) softmax probs\n","\n","    # 1) Squeeze off the last dim → (B, H, W)\n","    y_true = tf.squeeze(y_true, axis=-1)\n","\n","    # 2) One‑hot encode → (B, H, W, 5)\n","    y_true_o = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n","\n","    # 3) Flatten both to (B*H*W, 5)\n","    y_true_f = tf.reshape(y_true_o, [-1, num_classes])\n","    y_pred_f = tf.reshape(y_pred,   [-1, num_classes])\n","\n","    # 4) Compute intersection & union per class\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n","    union        = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n","\n","    # 5) Dice per class, then average\n","    dice_per_class = (2. * intersection + smooth) / (union + smooth)\n","    return tf.reduce_mean(dice_per_class)\n","\n","def dice_coefficient_loss(y_true, y_pred):\n","    return 1.0 - dice_coefficient(y_true, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"xj8ySx3ATPP9"},"source":["#Train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTVIxx_sD755","outputId":"b12fcf88-5e05-489d-f699-3f606234dde6","executionInfo":{"status":"ok","timestamp":1745609296626,"user_tz":-180,"elapsed":18460,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[GPU] Detected 1 GPU(s).\n","Train: 24, Val: 7, Test: 4\n"]}],"source":["# In a notebook cell, before any cv2 imports:\n","!pip install --quiet opencv-python-headless\n","import os\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","\n","# 1) Import TF first and set memory growth (fallback to experimental)\n","import tensorflow as tf\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    for gpu in gpus:\n","        try:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        except Exception as e:\n","            print(f\"[GPU] Could not set memory growth: {e}\")\n","    print(f\"[GPU] Detected {len(gpus)} GPU(s).\")\n","\n","# 2) Now import the rest\n","import numpy as np\n","import cv2\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import (\n","    ModelCheckpoint, CSVLogger, ReduceLROnPlateau,\n","    EarlyStopping, TensorBoard\n",")\n","from tensorflow.keras.optimizers import Adam\n","\n","\"\"\" Global parameters \"\"\"\n","Height = 512\n","Width  = 512\n","\n","def create_folder(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def load_dataset(path_dataset, val_ratio=0.2, test_ratio=0.1, random_state=42):\n","    img_dir  = os.path.join(path_dataset, \"images\")\n","    mask_dir = os.path.join(path_dataset, \"masks\")\n","\n","    image_exts = ('.png', '.jpg', '.jpeg')\n","    images = sorted([\n","        os.path.join(img_dir, f)\n","        for f in os.listdir(img_dir)\n","        if f.lower().endswith(image_exts)\n","    ])\n","\n","\n","    mask_exts = ('.tif', '.tiff')\n","    masks = sorted([\n","        os.path.join(mask_dir, f)\n","        for f in os.listdir(mask_dir)\n","        if f.lower().endswith(mask_exts)\n","    ])\n","\n","    if not images:\n","        raise RuntimeError(f\"No images found in {img_dir}\")\n","    if not masks:\n","        raise RuntimeError(f\"No masks found in {mask_dir}\")\n","\n","    images_lookup = {\n","        os.path.splitext(os.path.basename(p))[0]: p\n","        for p in images\n","    }\n","#MildDemented_0a7b1321-eba0-40dc-85b8-de34241554a2_jpg.rf.70370fcd9ac0fb82f91d27f558053d84.tif\n","    pairs = []\n","    for m in masks:\n","        base = os.path.splitext(os.path.basename(m))[0]\n","        if base.endswith('_mask'):\n","            root = base[:-5]\n","            img_path = images_lookup.get(root)\n","            if img_path:\n","                pairs.append((img_path, m))\n","\n","    if not pairs:\n","        raise RuntimeError(\"No matching image↔mask pairs found!\")\n","\n","    trainval, test = train_test_split(pairs, test_size=test_ratio, random_state=random_state)\n","    val_frac = val_ratio / (1.0 - test_ratio)\n","    train, val = train_test_split(trainval, test_size=val_frac, random_state=random_state)\n","\n","    train_imgs, train_masks = zip(*train)\n","    val_imgs,   val_masks   = zip(*val)\n","    test_imgs,  test_masks  = zip(*test)\n","\n","    return (\n","        list(train_imgs), list(train_masks),\n","        list(val_imgs),   list(val_masks),\n","        list(test_imgs),  list(test_masks)\n","    )\n","\n","def read_image_file(path_img):\n","    path = path_img.decode('utf-8') if isinstance(path_img, bytes) else path_img\n","    img = cv2.imread(path, cv2.IMREAD_COLOR)\n","    img = cv2.resize(img, (Width, Height)) / 255.0\n","    return img.astype(np.float32)\n","\n","def read_mask_file(path_mask):\n","    path = path_mask.decode('utf-8') if isinstance(path_mask, bytes) else path_mask\n","    mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","    if mask.ndim == 3:\n","        mask = mask[..., 0]\n","    mask = cv2.resize(mask, (Width, Height), interpolation=cv2.INTER_NEAREST)\n","\n","    # ✅ Clip to [0, 4], not [0, 3]:\n","    mask = np.clip(mask, 0, NUM_CLASSES-1)\n","\n","    # Convert to integer class IDs\n","    mask = mask.astype(np.int32)\n","\n","    # Add the channel dim back\n","    return mask[..., None]\n","\n","def tf_parse(path_img, path_mask):\n","    # Tell numpy_function: img→float32, mask→int32\n","    img, mask = tf.numpy_function(\n","        lambda pi, pm: (read_image_file(pi), read_mask_file(pm)),\n","        [path_img, path_mask],\n","        [tf.float32, tf.int32]          # <-- second output is now int32\n","    )\n","    img.set_shape([Height, Width, 3])\n","    mask.set_shape([Height, Width, 1])\n","    return img, mask\n","\n","def tf_dataset(inputs, targets, batch_size=2):\n","    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))\n","    ds = ds.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n","    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return ds\n","\n","def edit_mask_images(mask_dir, func, output_dir=None):\n","    out_dir = output_dir or mask_dir\n","    create_folder(out_dir)\n","    exts = ('.tif')\n","    for fname in os.listdir(mask_dir):\n","        if not fname.lower().endswith(exts):\n","            continue\n","        in_path  = os.path.join(mask_dir, fname)\n","        mask     = cv2.imread(in_path, cv2.IMREAD_UNCHANGED)\n","        edited   = func(mask)\n","        cv2.imwrite(os.path.join(out_dir, fname), edited)\n","    print(f\"[edit_mask_images] Applied edits to masks in {out_dir}\")\n","\n","if __name__ == \"__main__\":\n","    np.random.seed(42)\n","    tf.random.set_seed(42)\n","\n","    create_folder(\"/content/drive/MyDrive/Project_41725/files\")\n","\n","    batch_size = 16\n","    lr         = 1e-4\n","    epochs_num     = 100\n","    path_model = os.path.join(\"/content/drive/MyDrive/Project_41725/files\", \"model.h5\")\n","    path_csv   = os.path.join(\"/content/drive/MyDrive/Project_41725/files\", \"log.csv\")\n","\n","    ds_path = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg\"\n","    train_imgs, train_masks, val_imgs, val_masks, test_imgs, test_masks = load_dataset(ds_path)\n","\n","    print(f\"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")\n","\n","    train_ds = tf_dataset(train_imgs, train_masks, batch_size=batch_size)\n","    val_ds   = tf_dataset(val_imgs,   val_masks,   batch_size=batch_size)\n","    test_ds  = tf_dataset(test_imgs,  test_masks,  batch_size=batch_size)\n","\n","    # Build, compile and fit your model below:\n","    # model = Unet_model((Height, Width, 3))\n","    # model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coeff])\n","    # callbacks = [ModelCheckpoint(...), ...]\n","    # model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks)\n"]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","\n","# TODO: update this to your actual masks directory\n","mask_dir = \"/content/drive/MyDrive/Project_41725/newdata/train_maskimg/masks\"\n","\n","# Gather all .tif/.tiff files\n","mask_files = [\n","    f for f in os.listdir(mask_dir)\n","    if f.lower().endswith(('.tif', '.tiff'))\n","]\n","\n","print(f\"Found {len(mask_files)} mask files in:\\n  {mask_dir}\\n\")\n","\n","suffix_count = 0\n","\n","for fname in sorted(mask_files):\n","    path = os.path.join(mask_dir, fname)\n","    mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","    if mask is None:\n","        print(f\"[ERROR] Could not read {fname}\")\n","        continue\n","\n","    stem = os.path.splitext(fname)[0]\n","    has_mask_suffix = stem.endswith(\"_mask\")\n","    suffix_count += int(has_mask_suffix)\n","\n","    unique_vals = np.unique(mask)\n","    print(\n","        f\"{fname} | stem='{stem}' | '_mask' suffix? {has_mask_suffix} | \"\n","        f\"unique labels: {list(unique_vals)}\"\n","    )\n","\n","print(\n","    f\"\\nSummary:\\n\"\n","    f\"  With '_mask' suffix: {suffix_count}\\n\"\n","    f\"  Without suffix:      {len(mask_files) - suffix_count}\"\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"id":"fEtoUlCSeibu","executionInfo":{"status":"error","timestamp":1745414585669,"user_tz":-180,"elapsed":6941,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}},"outputId":"9eab300e-a82e-43cb-b879-cbd78a020d7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1738 mask files in:\n","  /content/drive/MyDrive/Project_41725/newdata/train_maskimg/masks\n","\n","MildDemented_00be34e4-c61c-45a8-8ee8-48e29a8adce0_jpg.rf.bc8e786abf4ec687946d6ebbbe11300e_mask.tif | stem='MildDemented_00be34e4-c61c-45a8-8ee8-48e29a8adce0_jpg.rf.bc8e786abf4ec687946d6ebbbe11300e_mask' | '_mask' suffix? True | unique labels: [np.uint8(0), np.uint8(4)]\n","MildDemented_01bb5650-386e-4bdd-854e-54e12030a3c0_jpg.rf.91288ed2e7955c1255997e31afe67b6f_mask.tif | stem='MildDemented_01bb5650-386e-4bdd-854e-54e12030a3c0_jpg.rf.91288ed2e7955c1255997e31afe67b6f_mask' | '_mask' suffix? True | unique labels: [np.uint8(0), np.uint8(4)]\n","MildDemented_01dc8d82-81d9-4344-a80e-cf981232df36_jpg.rf.f1975ff08e1b64dba4100aafd50aa3aa_mask.tif | stem='MildDemented_01dc8d82-81d9-4344-a80e-cf981232df36_jpg.rf.f1975ff08e1b64dba4100aafd50aa3aa_mask' | '_mask' suffix? True | unique labels: [np.uint8(0), np.uint8(4)]\n","MildDemented_02addcbc-619b-4a93-bca0-fb21e0ab7066_jpg.rf.5b10a3fa0c4f10c0b6cf689bf9766e1c_mask.tif | stem='MildDemented_02addcbc-619b-4a93-bca0-fb21e0ab7066_jpg.rf.5b10a3fa0c4f10c0b6cf689bf9766e1c_mask' | '_mask' suffix? True | unique labels: [np.uint8(0), np.uint8(4)]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-53894ef83eb7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[ERROR] Could not read {fname}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["for msk in train_masks:\n","    arr = cv2.imread(msk, cv2.IMREAD_UNCHANGED)\n","    if arr is None: continue\n","    if np.any(arr > 3):\n","        print(\"[INVALID]\", msk, np.unique(arr))\n"],"metadata":{"id":"3Dg7aU1H9yt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMrqwAWO8VHo","executionInfo":{"status":"error","timestamp":1745420004389,"user_tz":-180,"elapsed":7495,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}},"colab":{"base_uri":"https://localhost:8080/","height":383},"outputId":"3813ea14-4c3b-4369-c008-66e2b987576b"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-919299f6e2e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# 6) (Optional) Evaluate before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Before training:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# 7) Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_autograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Permanently allowed: %s: AutoGraph artifact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;31m# If this is a partial, unwrap it and redo all the checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mmulti_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 return tf.experimental.Optional.from_value(\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_autograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Permanently allowed: %s: AutoGraph artifact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;31m# If this is a partial, unwrap it and redo all the checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m    115\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1671\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1672\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4059\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4061\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_metrics_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\u001b[0m in \u001b[0;36mget_metrics_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mreturn_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0mreturn_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{mls.output_name}_{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_name_counters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                     \u001b[0munique_name_counters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/metrics/iou_metrics.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Only keep the target classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         true_positives = ops.take_along_axis(\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/numpy.py\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(x, indices, axis)\u001b[0m\n\u001b[1;32m   5382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTakeAlongAxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(x, indices, axis)\u001b[0m\n\u001b[1;32m   2121\u001b[0m     \u001b[0;31m# broadcast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m     \u001b[0mx_shape_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m     \u001b[0mindices_shape_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2124\u001b[0m     \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_scatter_nd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m     indices_shape = tf.tensor_scatter_nd_update(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    723\u001b[0m             return constant_op._tensor_shape_tensor_conversion_function(  # pylint: disable=protected-access\n\u001b[1;32m    724\u001b[0m                 input_shape)\n\u001b[0;32m--> 725\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    292\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_graph_constant\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;34m\"\"\"Create a graph constant and invoke constant callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    284\u001b[0m       tensor_util.make_tensor_proto(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import (\n","    ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",")\n","from tensorflow.keras.models import load_model\n","\n","# Hyper‑params & paths\n","NUM_CLASSES = 5\n","INPUT_SHAPE = (512, 512, 3)\n","EPOCHS      = 160\n","path_model  = \"/content/drive/MyDrive/Project_41725/files/model.h5\"\n","path_csv    = \"/content/drive/MyDrive/Project_41725/files/log.csv\"\n","\n","# 0) kill any existing graph/state\n","tf.keras.backend.clear_session()\n","\n","# 1) rebuild\n","model = build_unet(backbone_name='efficientnetb4', input_shape=(512,512,3), n_classes=5)\n","\n","# 2) override its internal name (strip out the '+')\n","model._name = \"unet\"\n","\n","# 3) Define losses & metrics\n","def dice_coef(y_true, y_pred, smooth=1e-6):\n","    y_true = tf.squeeze(y_true, axis=-1)\n","    y_true_o = tf.one_hot(tf.cast(y_true, tf.int32), depth=NUM_CLASSES)\n","    y_true_f = tf.reshape(y_true_o, [-1, NUM_CLASSES])\n","    y_pred_f = tf.reshape(y_pred,   [-1, NUM_CLASSES])\n","    inter = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n","    denom = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n","    dice_per_class = (2. * inter + smooth) / (denom + smooth)\n","    return tf.reduce_mean(dice_per_class)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1.0 - dice_coef(y_true, y_pred)\n","\n","def combined_loss(y_true, y_pred):\n","    ce = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n","    return ce + dice_loss(y_true, y_pred)\n","\n","class MeanIoUArgMax(tf.keras.metrics.MeanIoU):\n","    def __init__(self, num_classes=NUM_CLASSES, name=\"mean_iou\", dtype=None):\n","        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.squeeze(y_true, axis=-1)\n","        y_pred = tf.argmax(y_pred, axis=-1)\n","        return super().update_state(y_true, y_pred, sample_weight)\n","\n","# 4) Compile\n","model.compile(\n","    optimizer=Adam(1e-4),\n","    loss=combined_loss,\n","    metrics=[dice_coef, MeanIoUArgMax()]\n",")\n","\n","\n","# 5) Callbacks\n","callbacks = [\n","    ModelCheckpoint(path_model, verbose=1, save_best_only=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n","    CSVLogger(path_csv , append=False),\n","    EarlyStopping(monitor='val_loss', patience=20),\n","]\n","\n","# 6) (Optional) Evaluate before training\n","print(\"Before training:\", dict(zip(model.metrics_names, model.evaluate(test_ds))))\n","\n","# 7) Fit\n","model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=EPOCHS,\n","    callbacks=callbacks\n",")\n","# 6) Inference & metric collection\n","EVL_SCORE_list = []\n","for batch_images, batch_masks in test_ds:\n","    preds = model.predict(batch_images)\n","    # for each sample in the batch:\n","    for i in range(len(preds)):\n","        y_true = batch_masks[i].numpy().squeeze(-1)\n","        y_pred = np.argmax(preds[i], axis=-1)\n","        # compute F1, Jaccard, etc. (use sklearn.metrics)\n","        f1 = f1_score(y_true.flatten(), y_pred.flatten(), average='macro', zero_division=0)\n","        j  = jaccard_score(y_true.flatten(), y_pred.flatten(), average='macro', zero_division=0)\n","        r  = recall_score(y_true.flatten(), y_pred.flatten(), average='macro', zero_division=0)\n","        p  = precision_score(y_true.flatten(), y_pred.flatten(), average='macro', zero_division=0)\n","        img_name = \"batch{}_idx{}\".format(batch_index, i)  # or however you track filenames\n","        EVL_SCORE_list.append((img_name, f1, j, r, p))\n","\n","# 7) Save per-image metrics\n","df_scores = pd.DataFrame(EVL_SCORE_list,\n","                         columns=[\"Image\",\"F1\",\"Jaccard\",\"Recall\",\"Precision\"])\n","os.makedirs(os.path.dirname(score_csv), exist_ok=True)\n","df_scores.to_csv(score_csv, index=False)\n","print(\"Wrote\", score_csv)\n"]},{"cell_type":"markdown","metadata":{"id":"vDxYwAy1TSw-"},"source":["#Test\n"]},{"cell_type":"code","source":["from google.colab import drive\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","\n","# 1) Csv\n","import pandas as pd\n","\n","# suppose you build EVL_SCORE_list as [(img_name, f1, jaccard, recall, precision), …]\n","df = pd.DataFrame(\n","    EVL_SCORE_list,\n","    columns=[\"Image\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"]\n",")\n","df.to_csv(\n","    '/content/drive/MyDrive/DSP Project/files/score.csv',\n","    index=False\n",")\n","model_path = \"/content/drive/MyDrive/Project_41725/files/LinkNet/model.h5\"\n","\n","# 3) Load model for inference only—skip compile to avoid the TypeError\n","model = load_model(\n","    model_path,\n","    custom_objects={\n","        'dice_coefficient': dice_coefficient,\n","        'dice_coefficient_loss': dice_coefficient_loss\n","    },\n","    compile=False\n",")\n","\n","def predict_and_show(img_path, model, input_size=(512,512)):\n","    gray    = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","    resized = cv2.resize(gray, input_size)\n","    x       = np.stack([resized]*3, -1).astype(np.float32) / 255.0\n","\n","    # Run model\n","    pred = model.predict(x[None])[0]         # ➔ (H, W, num_classes)\n","    seg  = pred.argmax(-1).astype(np.uint8)  # ➔ (H, W)\n","\n","    # Compute and print class percentages\n","    unique, counts = np.unique(seg, return_counts=True)\n","    total = seg.size\n","    percentages = {int(cls): float(cnt/total*100) for cls, cnt in zip(unique, counts)}\n","    print(f\"\\nClass percentages for {img_path}:\")\n","    for cls, pct in percentages.items():\n","        print(f\"  Class {cls}: {pct:.2f}%\")\n","\n","    # Display images\n","    plt.figure(figsize=(8,4))\n","    plt.subplot(1,2,1)\n","    plt.title(\"Input\")\n","    plt.imshow(resized, cmap='gray')\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.title(\"Segmentation Overlay\")\n","    plt.imshow(resized, cmap='gray', alpha=0.5)\n","    plt.imshow(seg, cmap='jet',   alpha=0.5)\n","    plt.axis('off')\n","    plt.show()\n","\n","# 4) Upload & run on uploaded files\n","uploaded = files.upload()\n","for fn in uploaded:\n","    predict_and_show(fn, model)\n","    print(\"Num output channels:\", model.output_shape[-1])\n"],"metadata":{"id":"Z3W76IJ2KpaU","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1745425970566,"user_tz":-180,"elapsed":4050,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}},"outputId":"d47b8242-a7a2-4c57-e0e4-7a1bef3011fe"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'EVL_SCORE_list' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e97e024dc5ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# suppose you build EVL_SCORE_list as [(img_name, f1, jaccard, recall, precision), …]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m df = pd.DataFrame(\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mEVL_SCORE_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Jaccard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Recall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'EVL_SCORE_list' is not defined"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# -- ensure output folder exists --\n","out_dir = '/content/drive/MyDrive/DSP Project/files'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# 1) Load evaluation results\n","df = pd.read_csv(os.path.join(out_dir, 'score.csv'))\n","\n","# 2) (Optional) Inspect to confirm non-zero values\n","print(df.head(), '\\n', df.dtypes)\n","\n","# 3) Your plots…\n","plt.figure(figsize=(10,6))\n","for m in ['F1','Jaccard','Recall','Precision']:\n","    plt.hist(df[m], bins=20, alpha=0.5, label=m)\n","plt.legend(); plt.show()\n","\n","# 4) Compute & save summary stats\n","summary = pd.DataFrame({\n","    'Metric':  ['F1','Jaccard','Recall','Precision'],\n","    'Mean':    df[['F1','Jaccard','Recall','Precision']].mean().values,\n","    'Std Dev': df[['F1','Jaccard','Recall','Precision']].std().values\n","})\n","summary_path = os.path.join(out_dir, 'summary.csv')\n","summary.to_csv(summary_path, index=False)\n","print(f\"Saved summary to {summary_path}:\\n\", summary)\n"],"metadata":{"id":"kIkpIm1cMbSk","colab":{"base_uri":"https://localhost:8080/","height":987},"executionInfo":{"status":"ok","timestamp":1745418799871,"user_tz":-180,"elapsed":213,"user":{"displayName":"Abdlrahman Kobaissy","userId":"02284448119242242699"}},"outputId":"c327b7b4-6220-4b88-b485-8bffbf6d2593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   Unnamed: 0                                              Image   F1  \\\n","0           0  gg-221-_jpg.rf.0cf63f9303a5bab0b0066cf52be58c5...  0.0   \n","1           1  NonDemented_0b84884f-1949-4336-afa8-6f143c0f03...  0.0   \n","2           2  p-504-_jpg.rf.94f19ebd00e726b7be3e18b3c6c228db...  0.0   \n","3           3  m-134-_jpg.rf.18c248557418577de329f31428116467...  0.0   \n","4           4  62-15-_jpg.rf.0592a5a3648ad92b8da2c4125ed6313c...  0.0   \n","\n","   Jaccard  Recall  Precision  \n","0      0.0     0.0        0.0  \n","1      0.0     0.0        0.0  \n","2      0.0     0.0        0.0  \n","3      0.0     0.0        0.0  \n","4      0.0     0.0        0.0   \n"," Unnamed: 0      int64\n","Image          object\n","F1            float64\n","Jaccard       float64\n","Recall        float64\n","Precision     float64\n","dtype: object\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved summary to /content/drive/MyDrive/DSP Project/files/summary.csv:\n","       Metric  Mean  Std Dev\n","0         F1   0.0      0.0\n","1    Jaccard   0.0      0.0\n","2     Recall   0.0      0.0\n","3  Precision   0.0      0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"_xAvNp-n0SYO"},"source":["#Plot and Checks"]}],"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"af3817f455a048688649649dc2f4503e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5734b82e3ddf456b97a19bc7b669df2c","IPY_MODEL_59efbe06a0ff4360837cc2ceeb34bb38","IPY_MODEL_b69f5837b97542b287cc40f93176c473"],"layout":"IPY_MODEL_55eda1aacf304ec8a012e53b657c8410"}},"5734b82e3ddf456b97a19bc7b669df2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12ba4abe09d942e784b1bdb5c498d8ee","placeholder":"​","style":"IPY_MODEL_680d104643794616b9d0d7c0cbbb2fdb","value":"config.json: 100%"}},"59efbe06a0ff4360837cc2ceeb34bb38":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27c1467c072c485d8a15d69d88191c8b","max":6884,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73514f6a20ab4a5e89d2fd0b3dc45bf2","value":6884}},"b69f5837b97542b287cc40f93176c473":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_657a0fecb2f44634a0182d502bb7673a","placeholder":"​","style":"IPY_MODEL_803ecbc0461f430aa6625cac3f415407","value":" 6.88k/6.88k [00:00&lt;00:00, 796kB/s]"}},"55eda1aacf304ec8a012e53b657c8410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12ba4abe09d942e784b1bdb5c498d8ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680d104643794616b9d0d7c0cbbb2fdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27c1467c072c485d8a15d69d88191c8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73514f6a20ab4a5e89d2fd0b3dc45bf2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"657a0fecb2f44634a0182d502bb7673a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"803ecbc0461f430aa6625cac3f415407":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e70c768cc957404ca12fa5a0865b187e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5da413d85f5c4d9d85877db12d10e0d4","IPY_MODEL_a15e4440ae8f4e58af4e61626de2041d","IPY_MODEL_060172f07e194fe0aaf6ec82aa54e1fb"],"layout":"IPY_MODEL_a7bcda50c3cd4ceea5c85ec42f6b7c53"}},"5da413d85f5c4d9d85877db12d10e0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a484a4780691429fb1a9687a8811c6f2","placeholder":"​","style":"IPY_MODEL_9e8876065494457985a54ead4fa79013","value":"model.safetensors: 100%"}},"a15e4440ae8f4e58af4e61626de2041d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3130cf13836841be9722452f9bc80f92","max":15036944,"min":0,"orientation":"horizontal","style":"IPY_MODEL_478029251a294befa519149018ef4b14","value":15036944}},"060172f07e194fe0aaf6ec82aa54e1fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_161a9c4fe47246b7ad734b9de1a0c347","placeholder":"​","style":"IPY_MODEL_eb6c9f4374c646d78fe095e666cd29bc","value":" 15.0M/15.0M [00:00&lt;00:00, 143MB/s]"}},"a7bcda50c3cd4ceea5c85ec42f6b7c53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a484a4780691429fb1a9687a8811c6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e8876065494457985a54ead4fa79013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3130cf13836841be9722452f9bc80f92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"478029251a294befa519149018ef4b14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"161a9c4fe47246b7ad734b9de1a0c347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6c9f4374c646d78fe095e666cd29bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}